# Educational AI System - Parameter-Expert LLM Architecture

## üéØ Overview

This system transforms your original question generation script into a cutting-edge **parameter-specific expert LLM architecture** optimized for Manjaro Linux with AMD GPU (20GB VRAM). Instead of 4 large monolithic models, you now have **specialized expert LLMs** that focus on individual question parameters, creating a self-perfecting educational AI system.

## üèóÔ∏è Architecture Revolution

### Original System ‚Üí New Expert System
- **Before**: 4 large models (Mother, Principal, 3 Student Personas)
- **After**: 7+ specialized parameter experts + memory-optimized orchestration
- **Memory**: Max 2 concurrent models (10-11GB VRAM usage, 9-10GB for operations)
- **Processing**: Sequential parameter validation with async feedback loops

### Expert LLM Specialists
1. **Variation Expert** (`p.variation`) - Difficulty assessment specialist
2. **Taxonomy Expert** (`p.taxonomy_level`) - Bloom's taxonomy classifier  
3. **Math Expert** (`p.mathematical_requirement_level`) - Mathematical complexity analyzer
4. **Obstacle Expert** (`p.*_obstacle_*`) - Linguistic barrier detector
5. **Instruction Expert** (`p.instruction_*`) - Clarity and explicitness analyzer
6. **Content Expert** (`p.root_text_contains_irrelevant_information`) - Relevance validator
7. **Reference Expert** (`p.root_text_reference_explanatory_text`) - Text reference analyzer

## üöÄ Quick Start

### 1. Install ROCm and Dependencies
```bash
# Run the optimized setup script
chmod +x setup_rocm_Arch.sh
./setup_rocm_Arch.sh

# Reboot after installation
sudo reboot
```

### 2. Verify Installation
```bash
# Check GPU detection
rocm-smi

# Optimize GPU for compute workloads  
./optimize_gpu.sh

# Download required models
./download_models.sh
```

### 3. Start the System
```bash
# Start the complete system
python3 start_system.py

# Or start FastAPI directly
python3 -m uvicorn educational_ai_orchestrator:app --host 0.0.0.0 --port 8000
```

### 4. Test the System
```bash
# Run comprehensive tests
python3 test_system.py

# Access API documentation
# http://localhost:8000/docs
```

## üìä System Configuration

### Model Configuration (20GB VRAM Optimized)
```python
MODELS_CONFIG = {
    "llama3.1:8b": {"memory_gb": 5.5, "use": "Main generator + variation expert"},
    "mistral:7b": {"memory_gb": 5.0, "use": "Taxonomy + instruction expert"}, 
    "qwen2.5:7b": {"memory_gb": 5.0, "use": "Math expert"},
    "llama3.2:3b": {"memory_gb": 2.5, "use": "Lightweight parameter experts"}
}
```

### Memory Management Strategy
- **Semaphore Limit**: Max 2 concurrent models
- **Dynamic Loading**: Models swap in/out based on demand
- **VRAM Buffer**: 2GB reserved for operations
- **Automatic Cleanup**: Unused models unloaded automatically

## üß† Expert LLM Workflow

### Sequential Parameter Validation
```
Question Generation ‚Üí Parameter Expert 1 ‚Üí Parameter Expert 2 ‚Üí ... ‚Üí Parameter Expert N
     ‚Üë___________________ Feedback Loop (max 5 iterations) ___________________________|
```

### Processing Flow
1. **Main Generator**: Creates initial question using specialized prompt
2. **Parameter Experts**: Each expert validates their specific parameters
3. **Feedback Integration**: Experts provide improvement suggestions
4. **Iterative Refinement**: Question refined based on expert feedback
5. **Final Validation**: All parameters approved or max iterations reached
6. **CSV Export**: Question formatted for stakeholder requirements

## üîß API Endpoints

### Core Endpoints
- `POST /generate-question` - Generate single question with expert validation
- `POST /batch-generate` - Generate multiple questions efficiently  
- `GET /health` - System health check
- `GET /models/status` - Current VRAM usage and model status

### Example API Usage
```python
import aiohttp
import asyncio

async def generate_question():
    async with aiohttp.ClientSession() as session:
        request = {
            "topic": "Bed√ºrfnisse und G√ºter",
            "difficulty": "leicht", 
            "age_group": "9. Klasse",
            "context": "Wirtschaftliche Grundbegriffe"
        }
        
        async with session.post("http://localhost:8000/generate-question", json=request) as response:
            result = await response.json()
            return result

# Run the example
result = asyncio.run(generate_question())
print(f"Generated question in {result['total_processing_time']:.2f}s with {result['iterations']} iterations")
```

## üìã Parameter Expert Details

### Variation Expert (Difficulty Assessment)
- **Model**: llama3.1:8b
- **Focus**: `p.variation` (leicht/stammaufgabe/schwer)
- **Criteria**: Cognitive demand, complexity, context familiarity
- **Prompt**: `prompts/variation_expert.txt`

### Taxonomy Expert (Learning Objectives)
- **Model**: mistral:7b
- **Focus**: `p.taxonomy_level` (Stufe 1/Stufe 2)
- **Criteria**: Bloom's taxonomy classification, transfer requirements
- **Prompt**: `prompts/taxonomy_expert.txt`

### Math Expert (Quantitative Requirements)
- **Model**: qwen2.5:7b
- **Focus**: `p.mathematical_requirement_level` (0-2 scale)
- **Criteria**: Calculation complexity, mathematical representations
- **Prompt**: `prompts/math_expert.txt`

### Obstacle Expert (Linguistic Complexity)
- **Model**: llama3.2:3b  
- **Focus**: All `*_obstacle_*` parameters
- **Criteria**: Passive voice, negation, complex noun phrases
- **Prompt**: `prompts/obstacle_expert.txt`

## üéØ Compared to Original System

### Performance Improvements
| Metric | Original System | New Expert System |
|--------|----------------|-------------------|
| **Concurrent Models** | 4 large models | 2 optimized models |
| **VRAM Usage** | ~15-20GB | ~10-11GB |
| **Parameter Coverage** | Generic validation | Specialized experts |
| **Processing Speed** | Sequential bottleneck | Parallel validation |
| **Accuracy** | General knowledge | Domain expertise |

### Architectural Benefits
- **Specialized Expertise**: Each expert focuses on specific parameters
- **Memory Efficiency**: Smart model swapping prevents VRAM overflow
- **Scalable Design**: Easy to add new parameter experts
- **Better Validation**: Detailed, parameter-specific feedback
- **CSV Integration**: Direct output in stakeholder format

## üîç Monitoring and Debugging

### System Monitoring
```bash
# Monitor GPU usage
watch -n 1 rocm-smi

# Monitor system resources  
btop

# View logs
tail -f logs/educational_ai.log
```

### Performance Tuning
```bash
# GPU optimization for compute workloads
echo "manual" | sudo tee /sys/class/drm/card0/device/power_dpm_force_performance_level
echo "5" | sudo tee /sys/class/drm/card0/device/pp_power_profile_mode

# Memory optimization
echo 2097152 | sudo tee /proc/sys/vm/max_map_count
```

### Troubleshooting Common Issues
1. **ROCm Not Detected**: Check `HSA_OVERRIDE_GFX_VERSION` environment variable
2. **Models Not Loading**: Ensure Ollama service is running: `sudo systemctl status ollama`
3. **VRAM Overflow**: Reduce concurrent models or use smaller model variants
4. **Slow Response**: Check model download status with `ollama list`

## üìä Testing and Validation

### Comprehensive Test Suite
```bash
# Run all tests
python3 test_system.py

# Test specific components
python3 -c "import asyncio; from test_system import SystemTester; asyncio.run(SystemTester().test_health_endpoint())"
```

### Test Coverage
- ‚úÖ System health and connectivity
- ‚úÖ Model loading and memory management  
- ‚úÖ Single question generation with all parameters
- ‚úÖ Batch processing efficiency
- ‚úÖ Parameter expert coverage
- ‚úÖ Memory efficiency under load
- ‚úÖ CSV output format validation

## üöÄ Production Deployment

### Docker Deployment (Recommended)
```bash
# Build container
docker build -t educational-ai .

# Run with GPU support
docker run --device=/dev/kfd --device=/dev/dri \
  --group-add video -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  educational-ai
```

### Systemd Service
```bash
# Install as system service
sudo cp educational-ai.service /etc/systemd/system/
sudo systemctl enable educational-ai
sudo systemctl start educational-ai
```

## üéâ Next Steps

1. **Fine-tune Expert Prompts**: Customize `prompts/` directory for your specific needs
2. **Add More Experts**: Create specialists for additional parameters
3. **Performance Optimization**: Experiment with different model quantizations
4. **Integration**: Connect to your existing educational platform APIs
5. **Monitoring**: Set up Prometheus/Grafana for production monitoring

## üìö Resources

- **Original Script**: `BouncyQuestionCreator.py` (preserved for reference)
- **Expert Prompts**: `prompts/` directory
- **Test Reports**: Generated in `test_report.json` and `test_results.csv`
- **Model Management**: Handled automatically by Ollama
- **API Documentation**: http://localhost:8000/docs (when running)

---

**üéØ Your educational AI system is now a specialized, memory-efficient, parameter-expert architecture optimized for AMD GPU on Manjaro Linux. Each question parameter has its own expert, creating unprecedented quality and consistency in educational content generation.**